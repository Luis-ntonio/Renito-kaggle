{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation metric for Santa 2024.\"\"\"\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : DataFrame\n",
    "        DataFrame containing the original text in a column named 'text'.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    submission : DataFrame\n",
    "        DataFrame containing the permuted text in a column named 'text'.\n",
    "        Must have the same row IDs as the solution.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    row_id_column_name : str\n",
    "        Name of the column containing row IDs.\n",
    "        Ensures aligned comparison between solution and submission.\n",
    "\n",
    "    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n",
    "        Path to the serialized LLM.\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    clear_mem : bool, default=False\n",
    "        Clear GPU memory after scoring by clearing the CUDA cache.\n",
    "        Useful for testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean perplexity score. Lower is better.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the submission format is invalid or submitted strings are not valid permutations.\n",
    "    \"\"\"\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"\n",
    "    Calculates perplexity of text using a pre-trained language model.\n",
    "\n",
    "    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the pre-trained language model\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    device_map : str, default=\"auto\"\n",
    "        Device mapping for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of given texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_texts : str or list of str\n",
    "            A single string or a list of strings.\n",
    "\n",
    "        batch_size : int, default=None\n",
    "            Batch size for processing. Defaults to the number of input texts.\n",
    "\n",
    "        debug : bool, default=False\n",
    "            Print debugging information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or list of float\n",
    "            A single perplexity value if input is a single string,\n",
    "            or a list of perplexity values if input is a list of strings.\n",
    "        \"\"\"\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cc99a40d324a9288db701490a7b34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model_path = \"./model/gemma\"\n",
    "scorer = PerplexityCalculator(model_path=model_path, load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example \n",
    "``` py\n",
    "submission = pd.DataFrame({\n",
    "'id': [0, 1, 2],\n",
    "'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "})\n",
    "perplexities = scorer.get_perplexity(submission[\"text\"].tolist(), debug=True)\n",
    "\n",
    "sol_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "sol_counts\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = \"thsi is a slihgtly misspelled zr4g sentense\"\n",
    "sentence = \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge\"\n",
    "#sentence = \"A christmas sentence will appear after the dot. ornament elf gingerbread\"\n",
    "#sentence = \"This is a sentence: I am from Peru\"\n",
    "#sentence = \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\"\n",
    "#sentence = \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and\"\n",
    "encoded = scorer.tokenizer.encode(sentence, add_special_tokens=False)\n",
    "tokens = scorer.tokenizer.convert_ids_to_tokens(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = pd.read_csv(\"./Input/sample_submission.csv\")\n",
    "Iterative_sentences = data_input['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' gingerbread'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.tokenizer.decode(136507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def get_dictionaries(tokens, encoded):\n",
    "    dictionary = {}\n",
    "    init_tokens = {}\n",
    "    words = []\n",
    "    current_word = \"\"\n",
    "    idx = 0\n",
    "    corr_enc = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith('▁') or idx==0:\n",
    "            idx += 1\n",
    "\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                dictionary[current_word] = corr_enc[len(words) - 1]\n",
    "            # Remove the '▁' and start a new word\n",
    "            if idx != 1:\n",
    "                current_word = token[1:]\n",
    "            else: \n",
    "                current_word = token[0:]\n",
    "            corr_enc.append([encoded[i]])\n",
    "            init_tokens[encoded[i]] = current_word\n",
    "        else:\n",
    "            # Continue the existing word\n",
    "            corr_enc[len(words)].append(encoded[i])\n",
    "            current_word += token\n",
    "\n",
    "    # After the loop ends, add the last word if it exists\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        dictionary[current_word] = corr_enc[len(words) - 1]\n",
    "    return dictionary, init_tokens, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(dictionary):\n",
    "    available_words = list(dictionary.keys())\n",
    "\n",
    "    ppx_per_tuple = {}\n",
    "\n",
    "    for word in available_words:\n",
    "        remaining_words = [w for w in available_words if w != word]\n",
    "        ppx_per_tuple[word] = {}\n",
    "        for w in remaining_words:\n",
    "            tuple_score = scorer.get_perplexity(f\"A christmas sentence will appear after the dot. {word} {w}\")\n",
    "            ppx_per_tuple[word][w] = tuple_score\n",
    "    #sort the dictionary\n",
    "    ppx_per_tuple = {k: v for k, v in sorted(ppx_per_tuple.items(), key=lambda item: item[0])}\n",
    "\n",
    "    #sort sub dictionary for values\n",
    "    for key in ppx_per_tuple:\n",
    "        ppx_per_tuple[key] = {k: v for k, v in sorted(ppx_per_tuple[key].items(), key=lambda item: item[1])}\n",
    "    return ppx_per_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_value_sentence(data):\n",
    "    returns = []\n",
    "    all_words = set(data.keys())\n",
    "    \n",
    "    for word in all_words:\n",
    "        sentence = []\n",
    "        current_word = word\n",
    "        remaining_words = set([w for w in all_words if w != word])\n",
    "        sentence.append(current_word)\n",
    "\n",
    "        while len(remaining_words) > 0:\n",
    "            for k, v in data[current_word].items():\n",
    "                if k not in sentence:\n",
    "                    sentence.append(k)\n",
    "                    current_word = k\n",
    "                    remaining_words.remove(k)\n",
    "                    break\n",
    "        returns.append(\" \".join(sentence))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ornament scrooge mistletoe reindeer elf family advent gingerbread chimney fireplace\n",
      "reindeer scrooge mistletoe and the night elf family ornament gingerbread bake advent walk jump drive give laugh sleep chimney fireplace\n",
      "nutcracker yuletide cheer grinch holiday decorations ornament gifts stocking carol holly jingle sleigh polar beard nice naughty chimney workshop magi\n",
      "sleigh yuletide cheer unwrap the nutcracker grinch holiday decorations ornament gifts and of is nice carol sing jingle beard holly stocking chimney visit naughty polar magi workshop relax eat\n",
      "puzzle game poinsettia eggnog fruitcake snowglobe hohoho merry and you have to it star night of joy peace hope that the season greeting card from kaggle in as we wish not with candy peppermint chocolate milk cookie candle wreath bow toy doll paper angel dream believe wonder workshop fireplace wrapping\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(encoded)\n\u001b[1;32m      6\u001b[0m dictionary, init_tokens, words \u001b[38;5;241m=\u001b[39m get_dictionaries(tokens, encoded)\n\u001b[0;32m----> 7\u001b[0m ppx_per_tuple \u001b[38;5;241m=\u001b[39m \u001b[43mget_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m sentence \u001b[38;5;241m=\u001b[39m find_lowest_value_sentence(ppx_per_tuple)\n\u001b[1;32m     10\u001b[0m solution_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mget_tuples\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m      8\u001b[0m     ppx_per_tuple[word] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m remaining_words:\n\u001b[0;32m---> 10\u001b[0m         tuple_score \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA christmas sentence will appear after the dot. \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mword\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mw\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         ppx_per_tuple[word][w] \u001b[38;5;241m=\u001b[39m tuple_score\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#sort the dictionary\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 198\u001b[0m, in \u001b[0;36mPerplexityCalculator.get_perplexity\u001b[0;34m(self, input_texts, debug)\u001b[0m\n\u001b[1;32m    195\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get model output\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Shift logits and labels for calculating loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:1049\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:837\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    826\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    827\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    828\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m         cache_position,\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:567\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    565\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    566\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 567\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n\u001b[1;32m    569\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:86\u001b[0m, in \u001b[0;36mGemma2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:355\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    348\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    351\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    352\u001b[0m         ):\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    365\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    414\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 416\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for sen in Iterative_sentences:\n",
    "    encoded = scorer.tokenizer.encode(sen, add_special_tokens=False)\n",
    "    tokens = scorer.tokenizer.convert_ids_to_tokens(encoded)\n",
    "    dictionary, init_tokens, words = get_dictionaries(tokens, encoded)\n",
    "    ppx_per_tuple = get_tuples(dictionary)\n",
    "    sentence = find_lowest_value_sentence(ppx_per_tuple)\n",
    "\n",
    "    solution_final = \"\"\n",
    "    score = float(\"inf\")\n",
    "    for sol in sentence:\n",
    "        solutions = pd.DataFrame(\n",
    "            {'id': [0],\n",
    "            'text': sol})\n",
    "\n",
    "        perplexities = scorer.get_perplexity(solutions[\"text\"].tolist(), debug=False)\n",
    "        if perplexities[0] < score:\n",
    "            score = perplexities[0]\n",
    "            solution_final = sol\n",
    "    print(solution_final)\n",
    "    output.append(solution_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority queue not optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"ornament scrooge mistletoe reindeer elf family advent gingerbread chimney fireplace\"\n",
    ",\"reindeer scrooge mistletoe and the night elf family ornament gingerbread bake advent walk jump drive give laugh sleep chimney fireplace\"\n",
    ",\"nutcracker yuletide cheer grinch holiday decorations ornament gifts stocking carol holly jingle sleigh polar beard nice naughty chimney workshop magi\"\n",
    ",\"sleigh yuletide cheer unwrap the nutcracker grinch holiday decorations ornament gifts and of is nice carol sing jingle beard holly stocking chimney visit naughty polar magi workshop relax eat\"\n",
    ",\"puzzle game poinsettia eggnog fruitcake snowglobe hohoho merry and you have to it star night of joy peace hope that the season greeting card from kaggle in as we wish not with candy peppermint chocolate milk cookie candle wreath bow toy doll paper angel dream believe wonder workshop fireplace wrapping\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = pd.DataFrame(\n",
    "            {'id': [x for x in range(len(sentences))],\n",
    "            'text': sentences})\n",
    "\n",
    "perplexities = scorer.get_perplexity(solutions[\"text\"].tolist(), debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1083.8569951461084,\n",
       " 1815.118892385283,\n",
       " 688.941423011762,\n",
       " 867.5082310343188,\n",
       " 479.0835461252901]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(tokens: list, model, new_sentence):\n",
    "    new_sent = new_sentence.copy()\n",
    "    total_log_prob = 1.0\n",
    "    for tok in tokens: # p(scroge) = p(sc) * p(roo) * p(ge)\n",
    "        w = scorer.tokenizer.decode(tok)\n",
    "        text_with_special = f\"{scorer.tokenizer.bos_token}{' '.join(new_sentence)}{w}{scorer.tokenizer.eos_token}\"\n",
    "\n",
    "        minputs = scorer.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False,)\n",
    "        \n",
    "        outputs = scorer.model(**minputs)\n",
    "        logits = outputs.logits\n",
    "        shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "        shift_labels = minputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "        #applying softmax only to the 9+i token\n",
    "        probs = torch.softmax(shift_logits[0][-1], dim=0)\n",
    "\n",
    "        token_prob = probs[tok].item()\n",
    "\n",
    "        total_log_prob *= token_prob\n",
    "\n",
    "        # Append the chosen token to the current_ids for the next iteration\n",
    "        new_sent.append(w)\n",
    "    del minputs\n",
    "    del outputs\n",
    "    del logits\n",
    "    del shift_logits\n",
    "    del shift_labels\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import math\n",
    "import logging\n",
    "\n",
    "solutions = []\n",
    "words = list(dictionary.keys())\n",
    "\n",
    "priority_queue = []\n",
    "\n",
    "new_sentence = sentence.split()[:8]\n",
    "words_to_use = set(list(dictionary.keys())[8:])\n",
    "desire_sentence = set(encoded[9:])\n",
    "\n",
    "#save log to file\n",
    "logging.basicConfig(filename='search.log', level=logging.INFO)\n",
    "\n",
    "#start log\n",
    "logging.info(\"Starting search\")\n",
    "\n",
    "# Initial state\n",
    "heapq.heappush(priority_queue, (1.0, words_to_use.copy(), new_sentence.copy(), desire_sentence.copy()))\n",
    "\n",
    "while priority_queue:\n",
    "    # Pop the state with the smallest cost\n",
    "    cost, words_left, current_sentence, desired_left = heapq.heappop(priority_queue)\n",
    "\n",
    "    # If we've used all required words, we are done\n",
    "    if len(desired_left) == 0:\n",
    "        # Store the solution and break if desired\n",
    "        solutions.append(\" \".join(current_sentence))\n",
    "        break\n",
    "\n",
    "    # If no words left to use but still desired words remain, can't continue\n",
    "    if len(words_left) == 0:\n",
    "        continue\n",
    "\n",
    "    # Compute probabilities for all candidate next words\n",
    "    probs = {}\n",
    "    for key in words_left:\n",
    "        probs[key] = get_prob(dictionary[key], scorer.model, current_sentence)\n",
    "\n",
    "    # For each candidate word, create a new state\n",
    "    coef = -(len(encoded[9:]) - len(desired_left) + 3) \n",
    "\n",
    "    iter = True\n",
    "    for w in probs:\n",
    "        if (probs[w] * abs(cost)) > 10**coef:\n",
    "            iter = False   \n",
    "            new_sentence_state = current_sentence.copy()\n",
    "            new_sentence_state.append(w)\n",
    "\n",
    "            new_desired = desired_left.copy()\n",
    "            new_desired -= set(dictionary[w])  # Remove words contributed by w\n",
    "\n",
    "            new_words_left = words_left.copy()\n",
    "            new_words_left.remove(w)\n",
    "\n",
    "            # If we've achieved the desired sentence, we can record this solution\n",
    "            if len(new_desired) == 0:\n",
    "\n",
    "                solutions.append([\" \".join(new_sentence_state[8:]), - (probs[w] * abs(cost))])\n",
    "                logging.info(f\"Found sentence: {' '.join(new_sentence_state)}\")\n",
    "                # Depending on your strategy, you might break here or continue searching\n",
    "                break\n",
    "\n",
    "            # Push the new state into the priority queue\n",
    "            # Here cost is probs[w]. If you want to prioritize higher probabilities,\n",
    "            # you could use -probs[w].\n",
    "            logging.info(f\"Current sentence: {' '.join(new_sentence_state)}, Desired: {desired_left}, Words left: {words_left}, Cost: {- (probs[w] * abs(cost))}\")\n",
    "            \n",
    "            heapq.heappush(priority_queue, (- (probs[w] * abs(cost)), new_words_left, new_sentence_state, new_desired))\n",
    "    if iter:\n",
    "        #top 1 word\n",
    "        w = max(probs, key=probs.get)\n",
    "        new_sentence_state = current_sentence.copy()\n",
    "        new_sentence_state.append(w)\n",
    "\n",
    "        new_desired = desired_left.copy()\n",
    "        new_desired -= set(dictionary[w])\n",
    "\n",
    "        new_words_left = words_left.copy()\n",
    "        new_words_left.remove(w)\n",
    "\n",
    "        if len(new_desired) == 0:\n",
    "            solutions.append(\" \".join(new_sentence_state))\n",
    "            logging.info(f\"Found sentence: {' '.join(new_sentence_state)}\")\n",
    "            break\n",
    "\n",
    "        logging.info(f\"Current sentence: {' '.join(new_sentence_state)}, Desired: {desired_left}, Words left: {words_left}, Cost: {- (probs[w] * abs(cost))}\")\n",
    "\n",
    "        heapq.heappush(priority_queue, (- (probs[w] * abs(cost)), new_words_left, new_sentence_state, new_desired))\n",
    "# Convert solutions to a dataframe if needed\n",
    "#sort solutions by cost\n",
    "solutions = sorted(solutions, key=lambda x: x[1])\n",
    "solutions = [x[0] for x in solutions]\n",
    "solutions_df = pd.DataFrame(\n",
    "    {'id': range(len(solutions)),\n",
    "     'text': solutions}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "solutions = []\n",
    "words = list(dictionary.keys())\n",
    "\n",
    "for j in range(len(words) - 8):\n",
    "    new_sentence = sentence.split()[:8] + [words[8+j]]\n",
    "    words_to_use = set(list(dictionary.keys())[8:]) - set([words[8+j]])\n",
    "    init_tokens_ = set(list(init_tokens.keys())[8:]) - set(corr_enc[8+j])\n",
    "    desire_sentence = set(encoded[9:]) - set(dictionary[words[8+j]])\n",
    "    for i in tqdm(range(len(words) - 10)):\n",
    "        \n",
    "        probs = {}\n",
    "\n",
    "        for key in words_to_use:\n",
    "            probs[key] = get_prob(dictionary[key], scorer.model, new_sentence)\n",
    "\n",
    "        #get index of highest probability\n",
    "        best_word = max(probs, key=probs.get)\n",
    "\n",
    "        new_sentence.append(best_word)\n",
    "\n",
    "        desire_sentence -= set(dictionary[best_word])\n",
    "        words_to_use -= set([best_word])\n",
    "        if len(desire_sentence) == 0:\n",
    "            break\n",
    "    solutions.append(' '.join(new_sentence[8:]))\n",
    "\n",
    "solutions = pd.DataFrame(\n",
    "    {'id': range(len(solutions)),\n",
    "    'text': solutions})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "solutions = []\n",
    "words = list(dictionary.keys())\n",
    "for j in range(len(words) - 8):\n",
    "    new_sentence = sentence.split()[:8] + [words[8+j]]\n",
    "    init_tokens_ = set(list(init_tokens.keys())[8:]) - set(corr_enc[8+j])\n",
    "    desire_sentence = set(encoded[9:]) - set(dictionary[words[8+j]])\n",
    "    for i in tqdm(range(len(words) - 9)):\n",
    "        text_with_special = f\"{scorer.tokenizer.bos_token}{' '.join(new_sentence)}{scorer.tokenizer.eos_token}\"\n",
    "        minputs = scorer.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False,)\n",
    "        \n",
    "        outputs = scorer.model(**minputs)\n",
    "        logits = outputs.logits\n",
    "        shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "        shift_labels = minputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "        #applying softmax only to the 9+i token\n",
    "        #shift_logits[0][9+i] = F.softmax(shift_logits[0][9+i], dim=0)\n",
    "\n",
    "        for key, val in dictionary.items():\n",
    "            # val is array of tokens. do shift_logits[0][9+i][v] = prod (shift_logits[0][9+i][v] for v in val)\n",
    "            product_value = torch.prod(shift_logits[0][9+i][val])\n",
    "            for v in val:\n",
    "                shift_logits[0][9+i][v] = product_value\n",
    "                \n",
    "        max_index = torch.topk(shift_logits[0][9+i], 256000).indices #beggining of the sentence\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Find the lower index of all the words in desire_sentence\n",
    "        index = []\n",
    "        if len(desire_sentence) == 0:\n",
    "            break\n",
    "        \n",
    "        tok = 0\n",
    "        for enc in max_index:\n",
    "            if enc.item() in init_tokens_:\n",
    "                tok = enc.item()\n",
    "                init_tokens_ = init_tokens_ - set([tok])\n",
    "                break\n",
    "        #find the complete word in dictionary given the token\n",
    "        for word in dictionary:\n",
    "            if tok in dictionary[word]:\n",
    "                new_sentence = new_sentence + [word]\n",
    "                desire_sentence = desire_sentence - set(dictionary[word])\n",
    "                break\n",
    "\n",
    "        del minputs\n",
    "        del outputs\n",
    "        del logits\n",
    "        del shift_logits\n",
    "        del shift_labels\n",
    "\n",
    "    solutions.append(' '.join(new_sentence[8:]))\n",
    "\n",
    "solutions = pd.DataFrame(\n",
    "    {'id': range(len(solutions)),\n",
    "    'text': solutions})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2620.430116803095,\n",
       " 2811.311136149555,\n",
       " 1430.2792730137226,\n",
       " 1947.3383101792642,\n",
       " 2452.0691545936147,\n",
       " 1577.0026226114692,\n",
       " 3980.1011001356896,\n",
       " 1558.6299855556538,\n",
       " 4171.110140058703,\n",
       " 2198.020584446467]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions = pd.DataFrame(\n",
    "    {'id': [x for x in range(len(sentence))],\n",
    "    'text': sentence})\n",
    "\n",
    "# gingerbread ornament elf -> 105906.56265800883\n",
    "# gingerbread elf ornament -> 38657.65136955225\n",
    "# ornament gingerbread elf -> 182990.1307424248\n",
    "# ornament elf gingerbread -> 143630.59930807285\n",
    "# elf gingerbread ornament -> 9608176.378154187\n",
    "# elf ornament gingerbread -> 27371147.346616127\n",
    "\n",
    "perplexities = scorer.get_perplexity(solutions[\"text\"].tolist(), debug=False)\n",
    "perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First attempt using logist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "out = outputs['logits'][0,1:-1].tolist()\n",
    "out = torch.Tensor(out)\n",
    "out = F.softmax(out, dim=1)\n",
    "out = out.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "words = []\n",
    "current_word = \"\"\n",
    "idx = 0\n",
    "logits = []\n",
    "corr_enc = []\n",
    "for i, token in enumerate(tokens):\n",
    "    if token.startswith('▁') or idx==0:\n",
    "        idx += 1\n",
    "\n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "            logits[len(words) - 1] = [logits[len(words) - 1][j] * x for j, x in enumerate(out[i])]\n",
    "            corr_enc[len(words) - 1].append(encoded[i])\n",
    "        # Remove the '▁' and start a new word\n",
    "        if idx != 1:\n",
    "            current_word = token[1:]\n",
    "        else: \n",
    "            current_word = token[0:]\n",
    "        logits.append(out[i])\n",
    "        corr_enc.append([encoded[i]])\n",
    "    else:\n",
    "        # Continue the existing word\n",
    "        current_word += token\n",
    "\n",
    "# After the loop ends, add the last word if it exists\n",
    "if current_word:\n",
    "    words.append(current_word)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for x in logits:\n",
    "    probs.append([x[tok] for tok in encoded])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for row_idx, row in enumerate(probs):\n",
    "    for col_idx, value in enumerate(row):\n",
    "        token_str = int(encoded[col_idx])\n",
    "        if token_str not in dictionary:\n",
    "            dictionary[token_str] = [probs[row_idx][col_idx]]\n",
    "        else:\n",
    "            dictionary[token_str].append(probs[row_idx][col_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_probs = {}\n",
    "for j in corr_enc:\n",
    "    key = str(j)\n",
    "    arr = [1 for _ in range(len(words))]\n",
    "    arr = [arr[i] * dictionary[x][i] for i in range(len(arr)) for x in j]\n",
    "    dict_probs[key] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {}\n",
    "for i, w in enumerate(words):\n",
    "    translation[str(corr_enc[i])] = w\n",
    "\n",
    "old_keys = list(dict_probs.keys())  \n",
    "for old_key in old_keys:\n",
    "    new_key = translation[old_key]\n",
    "    dict_probs[new_key] = dict_probs.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict_probs  # Your dictionary\n",
    "\n",
    "min_length = min(len(v) for v in data.values())\n",
    "\n",
    "sorted_keys = []\n",
    "remaining_keys = dict(data)  # make a copy we can modify\n",
    "\n",
    "for i in range(min_length):\n",
    "    candidates = []\n",
    "    for k, arr in remaining_keys.items():\n",
    "        # Determine which indices to average over\n",
    "        start = max(0, i - 2)\n",
    "        end = min(len(arr) - 2, i + 2)  # ensure we don't go out of range\n",
    "        \n",
    "        # If the array is too short to have these positions, skip\n",
    "        # Actually, if 'end' < 'start', it means no suitable window\n",
    "        if end < start:\n",
    "            continue\n",
    "        \n",
    "        # Compute the average over the window [start:end]\n",
    "        window_vals = arr[start:end+1]\n",
    "        mse_val = sum([x**2 for x in window_vals]) / len(window_vals)\n",
    "        avg_val = sum(window_vals) / len(window_vals)\n",
    "        \n",
    "        candidates.append((k, avg_val))\n",
    "    \n",
    "    if not candidates:\n",
    "        # No candidates available, break early\n",
    "        break\n",
    "    \n",
    "    # Find the key with the maximum average\n",
    "    best_key = max(candidates, key=lambda x: x[1])[0]\n",
    "    sorted_keys.append(best_key)\n",
    "    \n",
    "    # Remove the chosen key so it's not selected again\n",
    "    del remaining_keys[best_key]\n",
    "\n",
    "print(\"Sorted keys:\", sorted_keys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sequence = []\n",
    "remaining_words = set(words)\n",
    "\n",
    "while remaining_words:\n",
    "    best_word = None\n",
    "    best_ppl = float('inf')\n",
    "    # Try adding each remaining word next and compute perplexity\n",
    "    for w in remaining_words:\n",
    "        candidate_sequence = current_sequence + [w]\n",
    "        submission = pd.DataFrame({'id': [0], 'text': [\" \".join(candidate_sequence)]})\n",
    "        candidate_ppl = scorer.get_perplexity(submission[\"text\"].tolist(), debug=False)[0]\n",
    "        if candidate_ppl < best_ppl:\n",
    "            best_ppl = candidate_ppl\n",
    "            best_word = w\n",
    "    \n",
    "    current_sequence.append(best_word)\n",
    "    remaining_words.remove(best_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
